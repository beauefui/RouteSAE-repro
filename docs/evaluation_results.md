# SAE 模型评估对比报告

> 生成时间: 2026-02-04 07:13:18

## 评估配置

| 配置项 | 值 |
|--------|-----|
| 评估样本数 | 10,000 |
| Batch Size | 64 |
| Sequence Length | 512 |
| GPU | cuda:3 |

## 评估结果

| 指标 | TopK | RouteSAE | MLSAE | Vanilla | Gated | JumpReLU | Crosscoder | RandomK |
|------|------|----------|-------|---------|-------|----------|------------|---------|
| **NormMSE** ↓ | 0.1937 | 0.2129 | 0.2510 | 0.0080 | 0.0077 | 0.1167 | 0.2487 | 0.8638 |
| **KL Divergence** ↓ | 371.16 | 124.58 | 232.46 | 14.86 | 14.42 | 233.35 | N/A | 3672.05 |


> 注: ↓ 表示越低越好

## 模型说明

1.  **TopK**: 标准 SAE，在 Layer 16 训练并评估。
2.  **RouteSAE**: 多层路由 SAE，动态选择 Layer 4-12。
3.  **MLSAE**: 多层 SAE，在所有层上训练统一的特征空间。
4.  **Vanilla**: 传统 SAE，使用 ReLU 和 L1 正则化。
5.  **Gated**: Gated SAE，分离 Gate 和 Magnitude。
6.  **JumpReLU**: JumpReLU SAE，使用与特征相关的可学习阈值。
7.  **Crosscoder**: 跨层稀疏编码器，学习多层共享特征 (NormMSE only)。
8.  **RandomK**: 使用 TopK 模型，但评估时随机选择层 (Baseline)。

## 详细对比分析

根据上述评估数据，结合 RouteSAE 论文及稀疏自编码器 (SAE) 的相关研究，我们对各个模型的性能进行深入分析。

### 2. 深度对比分析

#### 2.1 有效性验证 (The Validity Check)
*   **RandomK 作为基线**: 
    *   RandomK 的 **NormMSE (0.86)** 极高，接近 1.0 (完全无法重构)，说明随机投影丢失了绝大部分信息。
    *   **KLDiv (3672)** 更是天文数字，说明随机特征完全破坏了 LLM 的预测能力。
    *   **结论**: 这从反面验证了其他所有训练模型（NormMSE < 0.3）的有效性。它们确实学到了语言模型内部的特征结构，而非随机噪声。

#### 2.2 核心对决: RouteSAE vs TopK
这是验证 RouteSAE 论文核心贡献的关键对比组：
*   **重构能力 (NormMSE)**: RouteSAE (0.21) 略逊于 TopK (0.19)。
    *   **原因**: TopK 专注优化单一层 (Layer 16)，任务简单；RouteSAE 需要处理多层 (Layer 4-12)，且需要训练额外的 Router，拟合难度更大。
*   **语义功能 (KL Divergence)**: **RouteSAE (124.58) 显著优于 TopK (371.16)**，误差降低了约 **66%**。
    *   **深度解读**: 这揭示了 LLM 内部特征具有显著的**层级特异性 (Layer Specificity)**。
        *   Layer 16 的特征不能很好地代替其他层的特征。
        *   RouteSAE 通过**动态路由 (Dynamic Routing)**，根据输入 Token 的语义自动选择最合适的层进行特征提取。这种机制让它捕捉到了更本质、更具泛化能力的语义特征，从而在介入模型时造成的干扰最小。

#### 2.3 “过于完美”的陷阱: Vanilla & Gated
*   **现象**: Vanilla 和 Gated 模型的 NormMSE 低至 **0.008**，KLDiv 低至 **14**，数据上“碾压”了其他所有模型。
*   **异常分析**: 
    *   在稀疏自编码器领域，如此低的重构误差通常是一个**危险信号**。它强烈暗示模型发生了 **“恒等映射崩溃” (Identity Mapping Collapse)**。
    *   **原理**: 当 L1 正则化系数过小，或者 Gate 机制未能有效关闭神经元时，SAE 可能学会了直接利用所有神经元（Dense）来复制输入，从而放弃了“稀疏分解”这一核心目标。
    *   **建议**: 对于这两个模型，目前的性能数据虽然好看，但可能缺乏可解释性价值。建议检查其平均激活神经元数量 (L0)，如果 L0 此值接近 Hidden Size，则需要大幅调大 L1 系数重新训练。

#### 2.4 多层处理策略: RouteSAE vs MLSAE vs Crosscoder
*   **MLSAE**: 试图用一套特征通吃所有层。KLDiv (232) 介于 TopK 和 RouteSAE 之间。说明**层间共享特征**是存在的，但不如**层间路由**有效。
*   **Crosscoder**: NormMSE (0.25) 与 MLSAE 相当。作为最复杂的架构（跨层求和+独立解码），它旨在捕捉“跨层组合特征”。虽然目前没有 KLDiv 数据，但其较高的重构误差暗示了拟合跨层非线性关系的难度。

### 3. 总结与应用建议

1.  **追求高性能解释**: 首选 **RouteSAE**。它在保持合理的重构质量的同时，提供了最佳的下游功能保持能力，最有可能提取到真实的语义特征。
2.  **快速探索**: 使用 **TopK**。结构简单，训练稳定，虽然 KLDiv 稍高，但对于单层分析已足够强大。
3.  **模型调试**: 需重点关注 **Vanilla/Gated** 的稀疏性设置，避免陷入恒等映射的局部最优解。
